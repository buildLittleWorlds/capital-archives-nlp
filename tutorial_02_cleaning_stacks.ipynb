{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Cleaning the Stacks\n",
    "\n",
    "## The Capital Archives — A Course in Natural Language Processing\n",
    "\n",
    "---\n",
    "\n",
    "*The water-school archivists are fastidious. \"A pristine text,\" they say, \"is like the seventy pools of Mirado—no dirt, no contamination, pure meaning flowing without obstruction.\" The stone-school disagrees, of course. They believe every smudge and error is part of the text's history, its hardness, its permanence.*\n",
    "\n",
    "*But for computational analysis, we need clean data. The Chief has assigned you to standardize the collection.*\n",
    "\n",
    "---\n",
    "\n",
    "In this tutorial, you will learn:\n",
    "- String manipulation in Python and pandas\n",
    "- Regular expressions for pattern matching\n",
    "- Common text cleaning operations\n",
    "- Handling missing and inconsistent data\n",
    "- Building a reusable text preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# COLAB SETUP - Run this cell first!\n# ============================================\n# This cell sets up the environment for Google Colab\n# Skip this cell if running locally\n\nimport os\n\n# Clone the repository if running in Colab\nif 'google.colab' in str(get_ipython()):\n    if not os.path.exists('capital-archives-nlp'):\n        !git clone https://github.com/buildLittleWorlds/capital-archives-nlp.git\n    os.chdir('capital-archives-nlp')\n    print(\"✓ Repository cloned and ready!\")\nelse:\n    print(\"✓ Running locally - no setup needed\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Load our data\n",
    "manuscripts = pd.read_csv('manuscripts.csv')\n",
    "texts = pd.read_csv('manuscript_texts.csv')\n",
    "\n",
    "print(f\"Loaded {len(manuscripts)} manuscript records\")\n",
    "print(f\"Loaded {len(texts)} text sections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Understanding Your Text Data\n",
    "\n",
    "Before cleaning, we need to understand what we're working with. What kinds of issues might exist in our texts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine a sample of our texts\n",
    "sample_texts = texts.sample(5, random_state=42)\n",
    "\n",
    "for _, row in sample_texts.iterrows():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Manuscript: {row['manuscript_id']}, Section: {row['section']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(row['text'][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in texts:\")\n",
    "print(texts.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in manuscripts:\")\n",
    "print(manuscripts.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Basic String Operations\n",
    "\n",
    "pandas provides powerful string methods through the `.str` accessor. Let's explore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a working copy of one text\n",
    "sample_text = texts.iloc[0]['text']\n",
    "print(\"Original text (first 300 chars):\")\n",
    "print(sample_text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase conversion\n",
    "lower_text = sample_text.lower()\n",
    "print(\"Lowercased:\")\n",
    "print(lower_text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uppercase conversion\n",
    "upper_text = sample_text.upper()\n",
    "print(\"Uppercased:\")\n",
    "print(upper_text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String replacement\n",
    "# Let's replace line breaks with spaces\n",
    "cleaned = sample_text.replace('\\n', ' ')\n",
    "print(\"With line breaks removed:\")\n",
    "print(cleaned[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying string methods to a whole column\n",
    "# Count the length of each text\n",
    "texts['char_count'] = texts['text'].str.len()\n",
    "texts['word_count'] = texts['text'].str.split().str.len()\n",
    "\n",
    "print(\"Text statistics:\")\n",
    "print(texts[['manuscript_id', 'char_count', 'word_count']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Regular Expressions: The Archivist's Pattern-Finder\n",
    "\n",
    "Regular expressions (regex) let us find and manipulate patterns in text. They're essential for text cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic regex patterns\n",
    "test_text = \"Grigsu wrote MS-0012 in the year 869. He also wrote MS-0034, MS-0089, and MS-0093.\"\n",
    "\n",
    "# Find all manuscript IDs (pattern: MS- followed by digits)\n",
    "manuscript_ids = re.findall(r'MS-\\d+', test_text)\n",
    "print(f\"Manuscript IDs found: {manuscript_ids}\")\n",
    "\n",
    "# Find all years (3 or 4 digit numbers)\n",
    "years = re.findall(r'\\b\\d{3,4}\\b', test_text)\n",
    "print(f\"Years found: {years}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common regex patterns for text cleaning\n",
    "\n",
    "# Remove extra whitespace\n",
    "messy_text = \"This   has    extra   spaces    and\\n\\nnewlines.\"\n",
    "clean_text = re.sub(r'\\s+', ' ', messy_text).strip()\n",
    "print(f\"Before: '{messy_text}'\")\n",
    "print(f\"After: '{clean_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation (keeping apostrophes for contractions)\n",
    "text_with_punct = \"Hello! How are you? I'm fine, thanks.\"\n",
    "no_punct = re.sub(r\"[^\\w\\s']\", '', text_with_punct)\n",
    "print(f\"Before: '{text_with_punct}'\")\n",
    "print(f\"After: '{no_punct}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all words that start with capital letters (potential names/places)\n",
    "sample = \"Grigsu traveled from the Capital to Yeller Quarry with Yasho and Bagbu.\"\n",
    "capitalized = re.findall(r'\\b[A-Z][a-z]+\\b', sample)\n",
    "print(f\"Capitalized words: {capitalized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex Reference Card\n",
    "\n",
    "| Pattern | Meaning | Example |\n",
    "|---------|---------|----------|\n",
    "| `\\d` | Any digit | `\\d+` matches \"123\" |\n",
    "| `\\w` | Any word character | `\\w+` matches \"hello\" |\n",
    "| `\\s` | Any whitespace | `\\s+` matches spaces/tabs/newlines |\n",
    "| `.` | Any character | `a.c` matches \"abc\", \"aXc\" |\n",
    "| `*` | Zero or more | `ab*` matches \"a\", \"ab\", \"abbb\" |\n",
    "| `+` | One or more | `ab+` matches \"ab\", \"abbb\" but not \"a\" |\n",
    "| `?` | Zero or one | `ab?` matches \"a\" or \"ab\" |\n",
    "| `[]` | Character class | `[aeiou]` matches any vowel |\n",
    "| `^` | Start of string | `^Hello` matches \"Hello world\" |\n",
    "| `$` | End of string | `world$` matches \"Hello world\" |\n",
    "| `\\b` | Word boundary | `\\bcat\\b` matches \"cat\" but not \"catalog\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Practice with Regex\n",
    "\n",
    "Use regular expressions to extract information from this text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "practice_text = \"\"\"\n",
    "The debate took place on Day 15 of the Third Month, 869. \n",
    "Present were Grigsu, Yasho, Bagbu, and Mink. \n",
    "They discussed MS-0012, MS-0008, and MS-0045.\n",
    "Grigsu argued for 2 hours. Yasho spoke for 3 hours.\n",
    "The final vote was 7 to 5 in favor of the water-school.\n",
    "\"\"\"\n",
    "\n",
    "# YOUR CODE HERE: Find all manuscript IDs\n",
    "manuscript_ids = re.findall(r'MS-\\d+', practice_text)\n",
    "print(f\"Manuscript IDs: {manuscript_ids}\")\n",
    "\n",
    "# YOUR CODE HERE: Find all numbers\n",
    "\n",
    "# YOUR CODE HERE: Find all capitalized words (names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Building a Text Cleaning Function\n",
    "\n",
    "Let's create a reusable function that applies standard cleaning operations to any text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, lowercase=True, remove_punctuation=False, \n",
    "               normalize_whitespace=True, remove_numbers=False):\n",
    "    \"\"\"\n",
    "    Clean a text string with various options.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        The text to clean\n",
    "    lowercase : bool\n",
    "        Convert to lowercase\n",
    "    remove_punctuation : bool\n",
    "        Remove punctuation marks\n",
    "    normalize_whitespace : bool\n",
    "        Replace multiple spaces/newlines with single space\n",
    "    remove_numbers : bool\n",
    "        Remove numeric digits\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str : The cleaned text\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string if needed\n",
    "    text = str(text)\n",
    "    \n",
    "    # Normalize whitespace first\n",
    "    if normalize_whitespace:\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Lowercase\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    # Remove numbers\n",
    "    if remove_numbers:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove punctuation (but keep apostrophes)\n",
    "    if remove_punctuation:\n",
    "        text = re.sub(r\"[^\\w\\s']\", '', text)\n",
    "    \n",
    "    # Final whitespace cleanup\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the cleaning function\n",
    "test = \"\"\"ON THE PERMANENCE OF THE UTTERED\n",
    "By Grigsu Haldo\n",
    "\n",
    "A word is not a soft thing like water... It is HARD, harder than stone!\"\"\"\n",
    "\n",
    "print(\"Original:\")\n",
    "print(test)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Cleaned (default settings):\")\n",
    "print(clean_text(test))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Cleaned (with punctuation removed):\")\n",
    "print(clean_text(test, remove_punctuation=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning to our entire corpus\n",
    "texts['clean_text'] = texts['text'].apply(clean_text)\n",
    "\n",
    "# Compare original and cleaned\n",
    "sample_idx = 0\n",
    "print(\"Original (first 200 chars):\")\n",
    "print(texts.iloc[sample_idx]['text'][:200])\n",
    "print(\"\\nCleaned (first 200 chars):\")\n",
    "print(texts.iloc[sample_idx]['clean_text'][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Handling Author Name Variations\n",
    "\n",
    "The manuscripts have inconsistent author names. Let's standardize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What author names do we have?\n",
    "print(\"Unique authors:\")\n",
    "for author in sorted(manuscripts['author'].unique()):\n",
    "    print(f\"  {author}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the scholars table for canonical names\n",
    "scholars = pd.read_csv('scholars.csv')\n",
    "print(\"\\nScholars with alternative names:\")\n",
    "print(scholars[['name', 'also_known_as']].dropna(subset=['also_known_as']).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_author(author_name, scholars_df):\n",
    "    \"\"\"\n",
    "    Try to match an author name to a canonical scholar name.\n",
    "    \n",
    "    Returns the canonical name if found, otherwise returns the original.\n",
    "    \"\"\"\n",
    "    if pd.isna(author_name):\n",
    "        return 'Unknown'\n",
    "    \n",
    "    # Check for exact match\n",
    "    if author_name in scholars_df['name'].values:\n",
    "        return author_name\n",
    "    \n",
    "    # Check also_known_as column\n",
    "    for _, row in scholars_df.iterrows():\n",
    "        if pd.notna(row['also_known_as']):\n",
    "            aliases = [a.strip() for a in str(row['also_known_as']).split(',')]\n",
    "            if author_name in aliases:\n",
    "                return row['name']\n",
    "    \n",
    "    # No match found, return original\n",
    "    return author_name\n",
    "\n",
    "# Test it\n",
    "print(standardize_author('Bagbu', scholars))  # Should return canonical name\n",
    "print(standardize_author('Grigsu', scholars))  # Should return Grigsu Haldo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Creating a Clean Corpus\n",
    "\n",
    "Let's combine all our texts into a single, clean corpus ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate texts by manuscript (combining all sections)\n",
    "corpus = texts.groupby('manuscript_id').agg(\n",
    "    full_text=('text', ' '.join),\n",
    "    clean_text=('clean_text', ' '.join),\n",
    "    num_sections=('section', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Add word counts\n",
    "corpus['word_count'] = corpus['clean_text'].str.split().str.len()\n",
    "\n",
    "print(f\"Corpus contains {len(corpus)} documents\")\n",
    "print(f\"Total words: {corpus['word_count'].sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with metadata\n",
    "corpus = corpus.merge(\n",
    "    manuscripts[['manuscript_id', 'title', 'author', 'genre', 'authenticity_status']],\n",
    "    on='manuscript_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Sentence Segmentation\n",
    "\n",
    "For some analyses, we need to work with sentences rather than whole documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sentences(text):\n",
    "    \"\"\"\n",
    "    Split text into sentences using regex.\n",
    "    This is a simple approach - for better results, use nltk or spacy.\n",
    "    \"\"\"\n",
    "    # Split on sentence-ending punctuation followed by space and capital letter\n",
    "    # Or just split on .!? followed by whitespace\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    # Filter out empty strings\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "# Test it\n",
    "test_para = \"\"\"Grigsu believed words were hard like stones. \n",
    "Yasho disagreed completely! She argued that words dissolve. \n",
    "Who was right? The debate continues to this day.\"\"\"\n",
    "\n",
    "sentences = split_into_sentences(test_para)\n",
    "for i, s in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count sentences in each document\n",
    "corpus['num_sentences'] = corpus['full_text'].apply(lambda x: len(split_into_sentences(x)))\n",
    "corpus['avg_sentence_length'] = corpus['word_count'] / corpus['num_sentences']\n",
    "\n",
    "print(\"Sentence statistics:\")\n",
    "print(corpus[['manuscript_id', 'author', 'num_sentences', 'avg_sentence_length']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Who writes the longest sentences?\n",
    "author_sentence_length = corpus.groupby('author')['avg_sentence_length'].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"Average sentence length by author:\")\n",
    "print(author_sentence_length.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Saving Your Clean Data\n",
    "\n",
    "Let's save our cleaned corpus for use in future tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned corpus\n",
    "corpus.to_csv('corpus_cleaned.csv', index=False)\n",
    "print(f\"Saved cleaned corpus with {len(corpus)} documents\")\n",
    "\n",
    "# Also save the text sections with clean text\n",
    "texts[['manuscript_id', 'section', 'text', 'clean_text']].to_csv('texts_cleaned.csv', index=False)\n",
    "print(f\"Saved cleaned text sections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "1. **String operations**: `.lower()`, `.upper()`, `.replace()`, `.strip()`\n",
    "2. **Regular expressions**: Pattern matching with `re.findall()`, `re.sub()`\n",
    "3. **Text cleaning pipeline**: Building reusable cleaning functions\n",
    "4. **Name standardization**: Matching variations to canonical forms\n",
    "5. **Sentence segmentation**: Breaking text into sentences\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Clean early, clean consistently**: Apply the same cleaning to all texts\n",
    "- **Keep the original**: Always preserve the original text alongside cleaned versions\n",
    "- **Document your choices**: Text cleaning involves decisions that affect analysis\n",
    "- **Test your functions**: Verify cleaning works as expected on diverse inputs\n",
    "\n",
    "---\n",
    "\n",
    "*The water-school archivist inspects your work. \"Acceptable,\" she says, her tone suggesting this is high praise. \"The texts are clean, the metadata standardized. Now you may begin to count the words themselves.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 2.2: Custom Cleaning\n",
    "Write a function that removes all text within square brackets [like this], which often indicates editorial insertions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_brackets(text):\n",
    "    \"\"\"Remove all text within square brackets, including the brackets.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "test = \"Grigsu said [standing] that words are hard [the audience murmured].\"\n",
    "print(remove_brackets(test))  # Should print: \"Grigsu said that words are hard.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Find All Dates\n",
    "Write a regex to find all dates in the format \"Day X of the Y Month, YEAR\" from the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "date_pattern = r''  # Fill in the pattern\n",
    "\n",
    "# Test on the debate transcripts\n",
    "debate_texts = texts[texts['manuscript_id'].str.startswith('MS-01')]['text']\n",
    "for text in debate_texts:\n",
    "    dates = re.findall(date_pattern, text)\n",
    "    if dates:\n",
    "        print(f\"Found dates: {dates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4: Character Cleanup\n",
    "Some texts contain unusual characters from transcription errors. Write code to:\n",
    "1. Find all unique non-ASCII characters in the corpus\n",
    "2. Create a mapping to replace them with ASCII equivalents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "all_text = ' '.join(corpus['full_text'])\n",
    "\n",
    "# Find non-ASCII characters\n",
    "non_ascii = set(c for c in all_text if ord(c) > 127)\n",
    "print(f\"Non-ASCII characters found: {non_ascii}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}