{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4: Patterns in the Stone\n",
    "\n",
    "## The Capital Archives — A Course in Natural Language Processing\n",
    "\n",
    "---\n",
    "\n",
    "*\"The patterns are in the stone,\" Grigsu wrote in his final, fragmentary notes from the Yeller Quarry expedition. \"Not in single words but in their combinations. The sequences that repeat. The phrases that persist.\"*\n",
    "\n",
    "*The Chief wants you to find recurring phrases in the archive—patterns that appear again and again across different manuscripts.*\n",
    "\n",
    "---\n",
    "\n",
    "In this tutorial, you will learn:\n",
    "- N-grams: sequences of words\n",
    "- Collocations: words that appear together\n",
    "- Concordance: viewing words in context\n",
    "- Using NLTK's text analysis tools"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# COLAB SETUP - Run this cell first!\n# ============================================\n# This cell sets up the environment for Google Colab\n# Skip this cell if running locally\n\nimport os\n\n# Clone the repository if running in Colab\nif 'google.colab' in str(get_ipython()):\n    if not os.path.exists('capital-archives-nlp'):\n        !git clone https://github.com/buildLittleWorlds/capital-archives-nlp.git\n    os.chdir('capital-archives-nlp')\n    \n    # Install/download NLTK data\n    import nltk\n    nltk.download('punkt', quiet=True)\n    nltk.download('punkt_tab', quiet=True)\n    nltk.download('stopwords', quiet=True)\n    print(\"✓ Repository cloned and NLTK data downloaded!\")\nelse:\n    print(\"✓ Running locally - no setup needed\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk import ngrams, bigrams, trigrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
    "from nltk.text import Text\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "print(\"Libraries loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare corpus\n",
    "manuscripts = pd.read_csv('data/manuscripts.csv')\n",
    "texts = pd.read_csv('data/manuscript_texts.csv')\n",
    "\n",
    "# Create corpus\n",
    "corpus = texts.groupby('manuscript_id').agg(\n",
    "    text=('text', ' '.join)\n",
    ").reset_index()\n",
    "\n",
    "corpus = corpus.merge(\n",
    "    manuscripts[['manuscript_id', 'title', 'author', 'genre']],\n",
    "    on='manuscript_id', how='left'\n",
    ")\n",
    "\n",
    "# Combine all text\n",
    "all_text = ' '.join(corpus['text'])\n",
    "print(f\"Total characters in corpus: {len(all_text):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 What Are N-grams?\n",
    "\n",
    "An **n-gram** is a contiguous sequence of n items (words, in our case) from a text.\n",
    "\n",
    "- **Unigrams** (n=1): individual words\n",
    "- **Bigrams** (n=2): pairs of consecutive words\n",
    "- **Trigrams** (n=3): triples of consecutive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentence\n",
    "sample = \"Words are hard like stones in the village.\"\n",
    "tokens = word_tokenize(sample.lower())\n",
    "\n",
    "print(\"Tokens (unigrams):\")\n",
    "print(tokens)\n",
    "\n",
    "print(\"\\nBigrams:\")\n",
    "print(list(bigrams(tokens)))\n",
    "\n",
    "print(\"\\nTrigrams:\")\n",
    "print(list(trigrams(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create n-grams for the whole corpus\n",
    "all_tokens = word_tokenize(all_text.lower())\n",
    "# Filter to alphabetic tokens only\n",
    "all_tokens = [t for t in all_tokens if t.isalpha()]\n",
    "\n",
    "# Get bigrams and trigrams\n",
    "corpus_bigrams = list(bigrams(all_tokens))\n",
    "corpus_trigrams = list(trigrams(all_tokens))\n",
    "\n",
    "print(f\"Total bigrams: {len(corpus_bigrams):,}\")\n",
    "print(f\"Total trigrams: {len(corpus_trigrams):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count bigram frequencies\n",
    "bigram_freq = Counter(corpus_bigrams)\n",
    "\n",
    "print(\"Most common bigrams:\")\n",
    "for bg, count in bigram_freq.most_common(20):\n",
    "    print(f\"  {' '.join(bg)}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "Many common bigrams are just combinations of stopwords (\"of the\", \"in the\", \"to the\"). Let's filter those out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter bigrams: require at least one content word\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "content_bigrams = [(w1, w2) for w1, w2 in corpus_bigrams \n",
    "                   if w1 not in stop_words or w2 not in stop_words]\n",
    "\n",
    "content_bigram_freq = Counter(content_bigrams)\n",
    "\n",
    "print(\"Most common content bigrams:\")\n",
    "for bg, count in content_bigram_freq.most_common(20):\n",
    "    print(f\"  {' '.join(bg)}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even stricter: require both words to be content words\n",
    "strict_content_bigrams = [(w1, w2) for w1, w2 in corpus_bigrams \n",
    "                          if w1 not in stop_words and w2 not in stop_words]\n",
    "\n",
    "strict_bigram_freq = Counter(strict_content_bigrams)\n",
    "\n",
    "print(\"Most common bigrams (both words content):\")\n",
    "for bg, count in strict_bigram_freq.most_common(20):\n",
    "    print(f\"  {' '.join(bg)}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Trigrams and Beyond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigram analysis\n",
    "trigram_freq = Counter(corpus_trigrams)\n",
    "\n",
    "print(\"Most common trigrams:\")\n",
    "for tg, count in trigram_freq.most_common(15):\n",
    "    print(f\"  {' '.join(tg)}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content trigrams (at least 2 content words)\n",
    "def count_content_words(ngram, stopwords_set):\n",
    "    return sum(1 for w in ngram if w not in stopwords_set)\n",
    "\n",
    "content_trigrams = [tg for tg in corpus_trigrams \n",
    "                    if count_content_words(tg, stop_words) >= 2]\n",
    "\n",
    "content_trigram_freq = Counter(content_trigrams)\n",
    "\n",
    "print(\"Most common content trigrams:\")\n",
    "for tg, count in content_trigram_freq.most_common(15):\n",
    "    print(f\"  {' '.join(tg)}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Collocations: Statistically Significant Pairs\n",
    "\n",
    "Simple frequency counting finds common phrases, but some of those are common just because the individual words are common.\n",
    "\n",
    "**Collocations** are word combinations that occur together more often than chance would predict. NLTK provides statistical measures to find them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find collocations using PMI (Pointwise Mutual Information)\n",
    "# PMI measures how much more likely two words appear together than independently\n",
    "\n",
    "bigram_finder = BigramCollocationFinder.from_words(all_tokens)\n",
    "\n",
    "# Filter to bigrams that appear at least 3 times\n",
    "bigram_finder.apply_freq_filter(3)\n",
    "\n",
    "# Find best collocations by PMI\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "\n",
    "print(\"Top collocations by PMI:\")\n",
    "for colloc in bigram_finder.nbest(bigram_measures.pmi, 20):\n",
    "    print(f\"  {' '.join(colloc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also try likelihood ratio - often gives better results\n",
    "print(\"Top collocations by likelihood ratio:\")\n",
    "for colloc in bigram_finder.nbest(bigram_measures.likelihood_ratio, 20):\n",
    "    print(f\"  {' '.join(colloc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigram collocations\n",
    "trigram_finder = TrigramCollocationFinder.from_words(all_tokens)\n",
    "trigram_finder.apply_freq_filter(3)\n",
    "\n",
    "trigram_measures = TrigramAssocMeasures()\n",
    "\n",
    "print(\"Top trigram collocations by likelihood ratio:\")\n",
    "for colloc in trigram_finder.nbest(trigram_measures.likelihood_ratio, 15):\n",
    "    print(f\"  {' '.join(colloc)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Concordance: Words in Context\n",
    "\n",
    "A **concordance** shows every occurrence of a word in context—the words that appear before and after it. This helps us understand how a word is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an NLTK Text object for concordance\n",
    "nltk_text = Text(all_tokens)\n",
    "\n",
    "# Show concordance for a key term\n",
    "print(\"Concordance for 'stone':\")\n",
    "nltk_text.concordance('stone', width=75, lines=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Concordance for 'dissolution':\")\n",
    "nltk_text.concordance('dissolution', width=75, lines=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Concordance for 'yeller':\")\n",
    "nltk_text.concordance('yeller', width=75, lines=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Similar Words\n",
    "\n",
    "NLTK can find words that appear in similar contexts—they may be synonyms or related concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find words used in similar contexts to 'word'\n",
    "print(\"Words similar to 'word':\")\n",
    "nltk_text.similar('word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Words similar to 'stone':\")\n",
    "nltk_text.similar('stone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common contexts: what contexts do two words share?\n",
    "print(\"Common contexts of 'stone' and 'water':\")\n",
    "nltk_text.common_contexts(['stone', 'water'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Author-Specific N-grams\n",
    "\n",
    "Do different authors have different recurring phrases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_ngrams(corpus_df, author, n=2, min_freq=2, num_results=15):\n",
    "    \"\"\"\n",
    "    Get the most common n-grams for a specific author.\n",
    "    \"\"\"\n",
    "    author_docs = corpus_df[corpus_df['author'] == author]\n",
    "    if len(author_docs) == 0:\n",
    "        return []\n",
    "    \n",
    "    author_text = ' '.join(author_docs['text'])\n",
    "    tokens = word_tokenize(author_text.lower())\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "    \n",
    "    author_ngrams = list(ngrams(tokens, n))\n",
    "    freq = Counter(author_ngrams)\n",
    "    \n",
    "    return [(ng, count) for ng, count in freq.most_common(num_results) \n",
    "            if count >= min_freq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare bigrams between authors\n",
    "authors_to_compare = ['Grigsu Haldo', 'Yasho Krent']\n",
    "\n",
    "for author in authors_to_compare:\n",
    "    print(f\"\\n{author}'s common bigrams:\")\n",
    "    author_bigrams = get_author_ngrams(corpus, author, n=2)\n",
    "    for bg, count in author_bigrams:\n",
    "        print(f\"  {' '.join(bg)}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Searching for Specific Patterns\n",
    "\n",
    "Sometimes we want to find specific types of phrases—for example, phrases containing \"Yeller\" or phrases about philosophical concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all bigrams containing a specific word\n",
    "def bigrams_containing(word, bigram_counter):\n",
    "    \"\"\"\n",
    "    Find all bigrams containing a specific word.\n",
    "    \"\"\"\n",
    "    word = word.lower()\n",
    "    matching = [(bg, count) for bg, count in bigram_counter.items()\n",
    "                if word in bg]\n",
    "    return sorted(matching, key=lambda x: -x[1])\n",
    "\n",
    "# Bigrams containing 'yeller'\n",
    "print(\"Bigrams containing 'yeller':\")\n",
    "for bg, count in bigrams_containing('yeller', bigram_freq)[:15]:\n",
    "    print(f\"  {' '.join(bg)}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigrams containing 'meaning'\n",
    "print(\"Bigrams containing 'meaning':\")\n",
    "for bg, count in bigrams_containing('meaning', bigram_freq)[:15]:\n",
    "    print(f\"  {' '.join(bg)}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 Visualizing N-gram Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top bigrams\n",
    "top_bigrams = strict_bigram_freq.most_common(20)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "labels = [' '.join(bg) for bg, _ in top_bigrams]\n",
    "counts = [count for _, count in top_bigrams]\n",
    "\n",
    "ax.barh(range(len(labels)), counts, color='steelblue')\n",
    "ax.set_yticks(range(len(labels)))\n",
    "ax.set_yticklabels(labels)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Frequency')\n",
    "ax.set_title('Most Common Content Bigrams in the Archive')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9 Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "1. **N-grams**: Sequences of n consecutive words\n",
    "2. **Collocations**: Statistically significant word pairs using PMI and likelihood ratio\n",
    "3. **Concordance**: Viewing words in their surrounding context\n",
    "4. **Similar words**: Finding words used in similar contexts\n",
    "5. **Pattern search**: Finding n-grams containing specific terms\n",
    "\n",
    "### Key Patterns Discovered\n",
    "\n",
    "Our analysis has revealed recurring phrases in the archive:\n",
    "- Philosophical terms often appear in specific combinations\n",
    "- Different authors have distinctive phrase patterns\n",
    "- Key concepts like \"stone\", \"water\", and \"words\" appear in characteristic contexts\n",
    "\n",
    "---\n",
    "\n",
    "*\"There,\" the stone-school archivist says, pointing at your bigram chart. \"Do you see? The patterns persist. The combinations recur. This is not random. This is structure.\" You're beginning to think he might have a point.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 4.1: Four-grams\n",
    "Extract four-grams from the corpus. What are the most common four-word phrases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2: Genre-Specific Collocations\n",
    "Compare the collocations in \"treatise\" documents versus \"debate_transcript\" documents. What phrases are distinctive to each genre?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.3: Concordance for Key Figures\n",
    "Create concordances for the names of key scholars (Grigsu, Yasho, Bagbu). How are they discussed in the archive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}