{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 10: The Forger's Hand\n",
    "\n",
    "## The Capital Archives — A Course in Natural Language Processing\n",
    "\n",
    "## Capstone Investigation\n",
    "\n",
    "---\n",
    "\n",
    "*The Chief Archivist calls you into her office. The door closes. She places three manuscripts on the desk.*\n",
    "\n",
    "*\"These appeared over the past decade,\" she says. \"Each claims to be a lost work by Grigsu Haldo. Each was 'discovered' under suspicious circumstances. And each rewrites the history of the stone-school.\"*\n",
    "\n",
    "*She taps the manuscripts. \"The Senate is interested. If these are genuine, they upend everything we thought we knew about Grigsu's final philosophy. If they're forgeries, someone has been systematically deceiving us. I need proof. Not suspicions—evidence.\"*\n",
    "\n",
    "*\"Use everything you've learned. Find the forger's hand.\"*\n",
    "\n",
    "---\n",
    "\n",
    "In this capstone, you will:\n",
    "- Combine techniques from all previous tutorials\n",
    "- Build a comprehensive analysis of suspected forgeries\n",
    "- Present evidence for or against authenticity\n",
    "- Draw conclusions about who may have created the forgeries"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# COLAB SETUP - Run this cell first!\n# ============================================\n# This cell sets up the environment for Google Colab\n# Skip this cell if running locally\n\nimport os\n\n# Clone the repository if running in Colab\nif 'google.colab' in str(get_ipython()):\n    if not os.path.exists('capital-archives-nlp'):\n        !git clone https://github.com/buildLittleWorlds/capital-archives-nlp.git\n    os.chdir('capital-archives-nlp')\n    \n    # Install/download NLTK data (this notebook uses all NLTK features)\n    import nltk\n    nltk.download('punkt', quiet=True)\n    nltk.download('punkt_tab', quiet=True)\n    nltk.download('stopwords', quiet=True)\n    nltk.download('averaged_perceptron_tagger', quiet=True)\n    nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n    nltk.download('vader_lexicon', quiet=True)\n    print(\"✓ Repository cloned and NLTK data downloaded!\")\nelse:\n    print(\"✓ Running locally - no setup needed\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded. Investigation ready to begin.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 The Evidence\n",
    "\n",
    "Let's examine what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data\n",
    "manuscripts = pd.read_csv('manuscripts.csv')\n",
    "texts = pd.read_csv('manuscript_texts.csv')\n",
    "scholars = pd.read_csv('scholars.csv')\n",
    "\n",
    "# Load forgery evidence if available\n",
    "try:\n",
    "    forgery_evidence = pd.read_csv('forgery_evidence.csv')\n",
    "    print(f\"Loaded forgery evidence: {len(forgery_evidence)} records\")\n",
    "except:\n",
    "    forgery_evidence = None\n",
    "    print(\"No forgery evidence file found\")\n",
    "\n",
    "# Create corpus\n",
    "corpus = texts.groupby('manuscript_id').agg(\n",
    "    text=('text', ' '.join)\n",
    ").reset_index()\n",
    "\n",
    "corpus = corpus.merge(\n",
    "    manuscripts[['manuscript_id', 'title', 'author', 'genre', 'authenticity_status',\n",
    "                 'date_composed', 'date_archived']],\n",
    "    on='manuscript_id', how='left'\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal documents in corpus: {len(corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the suspected forgeries\n",
    "suspected = corpus[corpus['authenticity_status'] == 'suspected_forgery']\n",
    "\n",
    "print(f\"Suspected forgeries: {len(suspected)}\")\n",
    "print(\"\\nSuspected manuscripts:\")\n",
    "for _, row in suspected.iterrows():\n",
    "    print(f\"  {row['manuscript_id']}: {row['title']}\")\n",
    "    print(f\"    Attributed to: {row['author']}\")\n",
    "    print(f\"    Claimed date: {row['date_composed']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find verified Grigsu documents for comparison\n",
    "grigsu_verified = corpus[\n",
    "    (corpus['author'] == 'Grigsu Haldo') & \n",
    "    (corpus['authenticity_status'] == 'verified')\n",
    "]\n",
    "\n",
    "print(f\"Verified Grigsu documents: {len(grigsu_verified)}\")\n",
    "for _, row in grigsu_verified.iterrows():\n",
    "    print(f\"  {row['manuscript_id']}: {row['title'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Vocabulary Analysis\n",
    "\n",
    "First line of investigation: Do the suspected forgeries use vocabulary consistent with authentic Grigsu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary_stats(texts_series, label=\"\"):\n",
    "    \"\"\"\n",
    "    Get vocabulary statistics for a set of texts.\n",
    "    \"\"\"\n",
    "    all_text = ' '.join(texts_series)\n",
    "    tokens = word_tokenize(all_text.lower())\n",
    "    tokens = [t for t in tokens if t.isalpha()]\n",
    "    \n",
    "    word_freq = Counter(tokens)\n",
    "    total_words = len(tokens)\n",
    "    unique_words = len(word_freq)\n",
    "    \n",
    "    return {\n",
    "        'label': label,\n",
    "        'total_words': total_words,\n",
    "        'unique_words': unique_words,\n",
    "        'ttr': unique_words / total_words if total_words > 0 else 0,\n",
    "        'word_freq': word_freq\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze vocabulary for different groups\n",
    "grigsu_auth_vocab = get_vocabulary_stats(grigsu_verified['text'], \"Authentic Grigsu\")\n",
    "suspected_vocab = get_vocabulary_stats(suspected['text'], \"Suspected Forgeries\")\n",
    "\n",
    "# Also compare to known water-school authors\n",
    "yasho_docs = corpus[corpus['author'] == 'Yasho Krent']\n",
    "yasho_vocab = get_vocabulary_stats(yasho_docs['text'], \"Yasho (Water-School)\")\n",
    "\n",
    "print(\"Vocabulary comparison:\")\n",
    "print(f\"\\n{'Group':<25} {'Words':<10} {'Unique':<10} {'TTR':<10}\")\n",
    "print(\"-\" * 55)\n",
    "for v in [grigsu_auth_vocab, suspected_vocab, yasho_vocab]:\n",
    "    print(f\"{v['label']:<25} {v['total_words']:<10} {v['unique_words']:<10} {v['ttr']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key term analysis: water-school vs stone-school vocabulary\n",
    "stone_school_terms = ['stone', 'permanent', 'persist', 'hard', 'casting', 'village', 'grandmother']\n",
    "water_school_terms = ['dissolution', 'dissolve', 'pool', 'flow', 'residue', 'collective', 'water']\n",
    "\n",
    "def count_term_frequency(word_freq, terms, total_words):\n",
    "    \"\"\"Count frequency of terms per 1000 words.\"\"\"\n",
    "    count = sum(word_freq.get(t, 0) for t in terms)\n",
    "    return count / total_words * 1000 if total_words > 0 else 0\n",
    "\n",
    "print(\"\\nSchool-specific vocabulary (per 1000 words):\")\n",
    "print(f\"\\n{'Group':<25} {'Stone-school':<15} {'Water-school':<15}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for v in [grigsu_auth_vocab, suspected_vocab, yasho_vocab]:\n",
    "    stone_freq = count_term_frequency(v['word_freq'], stone_school_terms, v['total_words'])\n",
    "    water_freq = count_term_frequency(v['word_freq'], water_school_terms, v['total_words'])\n",
    "    print(f\"{v['label']:<25} {stone_freq:<15.2f} {water_freq:<15.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed term analysis\n",
    "print(\"\\nDetailed term frequencies (per 1000 words):\")\n",
    "print(f\"\\n{'Term':<15} {'Auth. Grigsu':<15} {'Suspected':<15} {'Yasho':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "key_terms = ['dissolution', 'stone', 'pool', 'persist', 'word', 'meaning', 'flow']\n",
    "for term in key_terms:\n",
    "    g_freq = grigsu_auth_vocab['word_freq'].get(term, 0) / grigsu_auth_vocab['total_words'] * 1000\n",
    "    s_freq = suspected_vocab['word_freq'].get(term, 0) / suspected_vocab['total_words'] * 1000 if suspected_vocab['total_words'] > 0 else 0\n",
    "    y_freq = yasho_vocab['word_freq'].get(term, 0) / yasho_vocab['total_words'] * 1000 if yasho_vocab['total_words'] > 0 else 0\n",
    "    print(f\"{term:<15} {g_freq:<15.2f} {s_freq:<15.2f} {y_freq:<15.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Stylometric Analysis\n",
    "\n",
    "Second line of investigation: Do the suspected forgeries match Grigsu's writing style?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse stylometric features from Tutorial 7\n",
    "FUNCTION_WORDS = ['the', 'a', 'an', 'and', 'or', 'but', 'if', 'that', 'which', \n",
    "                   'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had',\n",
    "                   'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may',\n",
    "                   'might', 'must', 'shall', 'can', 'to', 'of', 'in', 'for', 'on',\n",
    "                   'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "                   'before', 'after', 'above', 'below', 'between', 'under', 'again',\n",
    "                   'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',\n",
    "                   'how', 'all', 'each', 'few', 'more', 'most', 'other', 'some', 'such',\n",
    "                   'no', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',\n",
    "                   'just', 'now', 'i', 'we', 'you', 'he', 'she', 'it', 'they', 'this']\n",
    "\n",
    "def extract_style_features(text):\n",
    "    \"\"\"Extract stylometric features from text.\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text.lower())\n",
    "    words_alpha = [w for w in words if w.isalpha()]\n",
    "    \n",
    "    if len(words_alpha) == 0:\n",
    "        return features\n",
    "    \n",
    "    # Sentence features\n",
    "    sent_lengths = [len(word_tokenize(s)) for s in sentences]\n",
    "    features['avg_sentence_length'] = np.mean(sent_lengths) if sent_lengths else 0\n",
    "    features['std_sentence_length'] = np.std(sent_lengths) if sent_lengths else 0\n",
    "    \n",
    "    # Word features\n",
    "    word_lengths = [len(w) for w in words_alpha]\n",
    "    features['avg_word_length'] = np.mean(word_lengths)\n",
    "    \n",
    "    # Vocabulary\n",
    "    features['type_token_ratio'] = len(set(words_alpha)) / len(words_alpha)\n",
    "    \n",
    "    # Function words\n",
    "    word_freq = Counter(words_alpha)\n",
    "    for fw in FUNCTION_WORDS[:20]:  # Top 20 function words\n",
    "        features[f'fw_{fw}'] = word_freq.get(fw, 0) / len(words_alpha) * 100\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for all relevant documents\n",
    "def get_group_features(df, label):\n",
    "    features_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        feats = extract_style_features(row['text'])\n",
    "        feats['manuscript_id'] = row['manuscript_id']\n",
    "        feats['group'] = label\n",
    "        features_list.append(feats)\n",
    "    return pd.DataFrame(features_list)\n",
    "\n",
    "auth_grigsu_features = get_group_features(grigsu_verified, 'Authentic Grigsu')\n",
    "suspected_features = get_group_features(suspected, 'Suspected Forgery')\n",
    "yasho_features = get_group_features(yasho_docs, 'Yasho')\n",
    "\n",
    "all_features = pd.concat([auth_grigsu_features, suspected_features, yasho_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare key style metrics\n",
    "style_metrics = ['avg_sentence_length', 'avg_word_length', 'type_token_ratio', \n",
    "                 'fw_the', 'fw_that', 'fw_is', 'fw_and']\n",
    "\n",
    "style_comparison = all_features.groupby('group')[style_metrics].mean().round(3)\n",
    "print(\"\\nStylometric comparison:\")\n",
    "print(style_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize style differences\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "metrics_to_plot = ['avg_sentence_length', 'avg_word_length', 'type_token_ratio', 'fw_the']\n",
    "titles = ['Average Sentence Length', 'Average Word Length', 'Type-Token Ratio', 'Frequency of \"the\"']\n",
    "\n",
    "for ax, metric, title in zip(axes.flat, metrics_to_plot, titles):\n",
    "    data = [all_features[all_features['group'] == g][metric].values for g in all_features['group'].unique()]\n",
    "    ax.boxplot(data, labels=all_features['group'].unique())\n",
    "    ax.set_title(title)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.suptitle('Stylometric Comparison', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Document Similarity Analysis\n",
    "\n",
    "Third line of investigation: Which documents are the suspected forgeries most similar to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF for similarity analysis\n",
    "tfidf = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(corpus['text'])\n",
    "\n",
    "# Calculate similarity matrix\n",
    "similarity = cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each suspected forgery, find most similar documents\n",
    "print(\"Similarity analysis of suspected forgeries:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for _, row in suspected.iterrows():\n",
    "    doc_idx = corpus[corpus['manuscript_id'] == row['manuscript_id']].index[0]\n",
    "    sims = similarity[doc_idx]\n",
    "    \n",
    "    # Get top similar docs (excluding self)\n",
    "    top_indices = sims.argsort()[::-1][1:6]\n",
    "    \n",
    "    print(f\"\\n{row['manuscript_id']}: {row['title'][:50]}...\")\n",
    "    print(f\"  Attributed to: {row['author']}\")\n",
    "    print(f\"  Most similar to:\")\n",
    "    \n",
    "    for idx in top_indices:\n",
    "        sim_doc = corpus.iloc[idx]\n",
    "        print(f\"    {sims[idx]:.3f} - {sim_doc['title'][:40]}... by {sim_doc['author']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Visualizing the Evidence\n",
    "\n",
    "Let's create a visualization showing where the suspected forgeries fall in document space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to 2D using PCA\n",
    "pca = PCA(n_components=2)\n",
    "coords = pca.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "corpus['pca_x'] = coords[:, 0]\n",
    "corpus['pca_y'] = coords[:, 1]\n",
    "\n",
    "# Categorize documents\n",
    "def categorize_doc(row):\n",
    "    if row['authenticity_status'] == 'suspected_forgery':\n",
    "        return 'Suspected Forgery'\n",
    "    elif row['author'] == 'Grigsu Haldo':\n",
    "        return 'Authentic Grigsu'\n",
    "    elif row['author'] == 'Yasho Krent':\n",
    "        return 'Yasho (Water-School)'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "corpus['category'] = corpus.apply(categorize_doc, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "colors = {'Authentic Grigsu': 'green', 'Suspected Forgery': 'red', \n",
    "          'Yasho (Water-School)': 'blue', 'Other': 'gray'}\n",
    "sizes = {'Authentic Grigsu': 150, 'Suspected Forgery': 200, \n",
    "         'Yasho (Water-School)': 150, 'Other': 50}\n",
    "\n",
    "for cat in ['Other', 'Yasho (Water-School)', 'Authentic Grigsu', 'Suspected Forgery']:\n",
    "    mask = corpus['category'] == cat\n",
    "    ax.scatter(corpus.loc[mask, 'pca_x'], corpus.loc[mask, 'pca_y'],\n",
    "               c=colors[cat], s=sizes[cat], label=cat, alpha=0.7)\n",
    "\n",
    "# Add labels for suspected forgeries\n",
    "for _, row in suspected.iterrows():\n",
    "    ax.annotate(row['manuscript_id'], (row['pca_x'], row['pca_y']),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.set_title('Document Space: Where Do the Suspected Forgeries Cluster?')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6 Examining the Forgery Evidence File\n",
    "\n",
    "If available, let's examine the pre-compiled evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if forgery_evidence is not None:\n",
    "    print(\"Forgery evidence from the archives:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Show evidence for suspected documents\n",
    "    for ms_id in suspected['manuscript_id'].values:\n",
    "        ms_evidence = forgery_evidence[forgery_evidence['manuscript_id'] == ms_id]\n",
    "        if len(ms_evidence) > 0:\n",
    "            print(f\"\\n{ms_id}:\")\n",
    "            for _, row in ms_evidence.iterrows():\n",
    "                print(f\"  [{row['evidence_type']}] {row['description']}\")\n",
    "else:\n",
    "    print(\"No forgery evidence file available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.7 Building the Case\n",
    "\n",
    "Let's summarize all the evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile evidence summary\n",
    "print(\"=\"*70)\n",
    "print(\"INVESTIGATION SUMMARY: THE SUSPECTED FORGERIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. VOCABULARY EVIDENCE\")\n",
    "print(\"-\"*40)\n",
    "print(\"The suspected forgeries show vocabulary patterns inconsistent with\")\n",
    "print(\"authentic Grigsu and more consistent with water-school writings.\")\n",
    "print(\"\\n   Key finding: Water-school terminology appears at rates\")\n",
    "print(\"   significantly higher than in authentic Grigsu texts.\")\n",
    "\n",
    "print(\"\\n2. STYLOMETRIC EVIDENCE\")\n",
    "print(\"-\"*40)\n",
    "print(\"The writing style of suspected forgeries differs from authentic Grigsu\")\n",
    "print(\"in measurable ways:\")\n",
    "if len(auth_grigsu_features) > 0 and len(suspected_features) > 0:\n",
    "    auth_sent_len = auth_grigsu_features['avg_sentence_length'].mean()\n",
    "    sus_sent_len = suspected_features['avg_sentence_length'].mean()\n",
    "    print(f\"   - Average sentence length: Authentic={auth_sent_len:.1f}, Suspected={sus_sent_len:.1f}\")\n",
    "\n",
    "print(\"\\n3. SIMILARITY EVIDENCE\")\n",
    "print(\"-\"*40)\n",
    "print(\"The suspected forgeries cluster closer to water-school documents\")\n",
    "print(\"than to authentic Grigsu texts in document space.\")\n",
    "\n",
    "print(\"\\n4. CONCLUSION\")\n",
    "print(\"-\"*40)\n",
    "print(\"Multiple lines of evidence suggest the suspected manuscripts are\")\n",
    "print(\"NOT authentic works of Grigsu Haldo. They appear to have been\")\n",
    "print(\"written by someone familiar with water-school philosophy who\")\n",
    "print(\"attempted to attribute stone-school recantations to Grigsu.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.8 Your Investigation Report\n",
    "\n",
    "Write your own investigation report based on the evidence you've gathered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigation Report Template\n",
    "\n",
    "**Date:** [Today's date]\n",
    "\n",
    "**Investigator:** [Your name]\n",
    "\n",
    "**Subject:** Authenticity of Manuscripts MS-0156, MS-0157, MS-0158, MS-0159\n",
    "\n",
    "---\n",
    "\n",
    "#### Executive Summary\n",
    "\n",
    "[Write 2-3 sentences summarizing your conclusions]\n",
    "\n",
    "#### Evidence Summary\n",
    "\n",
    "**Vocabulary Analysis:**\n",
    "- [List key findings]\n",
    "\n",
    "**Stylometric Analysis:**\n",
    "- [List key findings]\n",
    "\n",
    "**Document Similarity:**\n",
    "- [List key findings]\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "[Are the documents authentic? Why or why not?]\n",
    "\n",
    "#### Possible Forger\n",
    "\n",
    "[Based on the evidence, who might have created these documents?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.9 Conclusion\n",
    "\n",
    "In this capstone investigation, you:\n",
    "\n",
    "1. Combined **vocabulary analysis** to detect anachronistic or school-inappropriate terminology\n",
    "2. Applied **stylometric analysis** to compare writing patterns\n",
    "3. Used **document similarity** to see where texts cluster\n",
    "4. Synthesized **multiple lines of evidence** into a coherent argument\n",
    "\n",
    "These are the same techniques used by real forensic linguists to detect forgeries, identify authors, and authenticate disputed documents.\n",
    "\n",
    "---\n",
    "\n",
    "*The Chief reads your report. She nods slowly. \"Good work,\" she says. \"The Senate will want to see this. The Mink estate will probably protest. There will be a hearing.\"*\n",
    "\n",
    "*She taps the manuscripts. \"But the evidence is clear. These are not Grigsu's words. Someone tried to rewrite history.\"*\n",
    "\n",
    "*She looks at you. \"Welcome to the archives. The work continues.\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}