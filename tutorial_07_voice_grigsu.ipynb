{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 7: The Voice of Grigsu\n",
    "\n",
    "## The Capital Archives — A Course in Natural Language Processing\n",
    "\n",
    "---\n",
    "\n",
    "*Several manuscripts claim Grigsu Haldo as their author. Some were found among his personal papers after his death. Others appeared years later, \"discovered\" by various archivists. The Chief is skeptical of the later discoveries. \"A scholar's voice is distinctive,\" she says. \"Can you identify the imposters by measuring the voice?\"*\n",
    "\n",
    "*This is the problem of **authorship attribution**: determining who wrote an anonymous or disputed text.*\n",
    "\n",
    "---\n",
    "\n",
    "In this tutorial, you will learn:\n",
    "- Stylometric features for authorship\n",
    "- Function words as author fingerprints\n",
    "- Building a classifier for authorship\n",
    "- Evaluating and interpreting results"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# COLAB SETUP - Run this cell first!\n# ============================================\n# This cell sets up the environment for Google Colab\n# Skip this cell if running locally\n\nimport os\n\n# Clone the repository if running in Colab\nif 'google.colab' in str(get_ipython()):\n    if not os.path.exists('capital-archives-nlp'):\n        !git clone https://github.com/buildLittleWorlds/capital-archives-nlp.git\n    os.chdir('capital-archives-nlp')\n    \n    # Install/download NLTK data\n    import nltk\n    nltk.download('punkt', quiet=True)\n    nltk.download('punkt_tab', quiet=True)\n    nltk.download('averaged_perceptron_tagger', quiet=True)\n    nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n    print(\"✓ Repository cloned and NLTK data downloaded!\")\nelse:\n    print(\"✓ Running locally - no setup needed\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus\n",
    "manuscripts = pd.read_csv('data/manuscripts.csv')\n",
    "texts = pd.read_csv('data/manuscript_texts.csv')\n",
    "\n",
    "corpus = texts.groupby('manuscript_id').agg(\n",
    "    text=('text', ' '.join)\n",
    ").reset_index()\n",
    "\n",
    "corpus = corpus.merge(\n",
    "    manuscripts[['manuscript_id', 'title', 'author', 'genre', 'authenticity_status']],\n",
    "    on='manuscript_id', how='left'\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(corpus)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 What is Stylometry?\n",
    "\n",
    "**Stylometry** is the quantitative study of writing style. The key insight: \n",
    "\n",
    "> Authors have unconscious habits that are difficult to fake. The words they choose, the sentence structures they prefer, even how they use common words—all create a distinctive \"fingerprint.\"\n",
    "\n",
    "### Key Stylometric Features\n",
    "\n",
    "1. **Sentence length** (average, variation)\n",
    "2. **Word length** (average, distribution)\n",
    "3. **Vocabulary richness** (type-token ratio)\n",
    "4. **Function word frequencies** (the, and, of, to, ...)\n",
    "5. **POS tag distributions**\n",
    "6. **Punctuation patterns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define stylometric features\n",
    "\n",
    "# Common function words (author fingerprint)\n",
    "FUNCTION_WORDS = [\n",
    "    'the', 'a', 'an', 'and', 'or', 'but', 'if', 'then', 'because', 'as',\n",
    "    'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
    "    'between', 'into', 'through', 'during', 'before', 'after', 'above',\n",
    "    'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over',\n",
    "    'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when',\n",
    "    'where', 'why', 'how', 'all', 'each', 'few', 'more', 'most', 'other',\n",
    "    'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "    'than', 'too', 'very', 'can', 'will', 'just', 'should', 'now', 'i',\n",
    "    'we', 'you', 'he', 'she', 'it', 'they', 'what', 'which', 'who', 'whom',\n",
    "    'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were',\n",
    "    'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "    'did', 'doing', 'would', 'could', 'ought', 'might', 'must', 'shall'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stylometric_features(text):\n",
    "    \"\"\"\n",
    "    Extract stylometric features from a text.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Feature name -> value\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Tokenize\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text.lower())\n",
    "    words_alpha = [w for w in words if w.isalpha()]\n",
    "    \n",
    "    if len(words_alpha) == 0:\n",
    "        return {}\n",
    "    \n",
    "    # --- Sentence-level features ---\n",
    "    sentence_lengths = [len(word_tokenize(s)) for s in sentences]\n",
    "    if len(sentence_lengths) > 0:\n",
    "        features['avg_sentence_length'] = np.mean(sentence_lengths)\n",
    "        features['std_sentence_length'] = np.std(sentence_lengths)\n",
    "    else:\n",
    "        features['avg_sentence_length'] = 0\n",
    "        features['std_sentence_length'] = 0\n",
    "    \n",
    "    # --- Word-level features ---\n",
    "    word_lengths = [len(w) for w in words_alpha]\n",
    "    features['avg_word_length'] = np.mean(word_lengths)\n",
    "    features['std_word_length'] = np.std(word_lengths)\n",
    "    \n",
    "    # Vocabulary richness\n",
    "    features['type_token_ratio'] = len(set(words_alpha)) / len(words_alpha)\n",
    "    \n",
    "    # --- Function word frequencies ---\n",
    "    word_freq = Counter(words_alpha)\n",
    "    total_words = len(words_alpha)\n",
    "    \n",
    "    for fw in FUNCTION_WORDS:\n",
    "        features[f'fw_{fw}'] = word_freq.get(fw, 0) / total_words * 100\n",
    "    \n",
    "    # --- POS tag distribution ---\n",
    "    tagged = pos_tag(words_alpha[:1000])  # Limit for speed\n",
    "    pos_freq = Counter(tag for _, tag in tagged)\n",
    "    total_tags = len(tagged)\n",
    "    \n",
    "    for pos in ['NN', 'VB', 'JJ', 'RB', 'IN', 'DT', 'PRP']:\n",
    "        features[f'pos_{pos}'] = pos_freq.get(pos, 0) / total_tags * 100\n",
    "    \n",
    "    # --- Punctuation ---\n",
    "    features['question_ratio'] = text.count('?') / (len(sentences) + 1)\n",
    "    features['exclamation_ratio'] = text.count('!') / (len(sentences) + 1)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test feature extraction\n",
    "sample_doc = corpus.iloc[0]\n",
    "features = extract_stylometric_features(sample_doc['text'])\n",
    "\n",
    "print(f\"Features for '{sample_doc['title'][:40]}...':\")\n",
    "for name, value in list(features.items())[:15]:\n",
    "    print(f\"  {name}: {value:.3f}\")\n",
    "print(f\"  ... and {len(features) - 15} more features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for all documents\n",
    "all_features = []\n",
    "for _, row in corpus.iterrows():\n",
    "    feats = extract_stylometric_features(row['text'])\n",
    "    feats['manuscript_id'] = row['manuscript_id']\n",
    "    feats['author'] = row['author']\n",
    "    feats['authenticity'] = row['authenticity_status']\n",
    "    all_features.append(feats)\n",
    "\n",
    "features_df = pd.DataFrame(all_features)\n",
    "print(f\"Extracted {len(features_df.columns) - 3} features for {len(features_df)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Comparing Authors' Styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare key features across authors\n",
    "key_features = ['avg_sentence_length', 'avg_word_length', 'type_token_ratio',\n",
    "                'fw_the', 'fw_and', 'fw_that', 'fw_is']\n",
    "\n",
    "author_styles = features_df.groupby('author')[key_features].mean()\n",
    "\n",
    "# Show top authors\n",
    "top_authors = features_df['author'].value_counts().head(8).index\n",
    "print(\"Style comparison (top authors):\")\n",
    "print(author_styles.loc[top_authors].round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize style differences\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Filter to authors with enough documents\n",
    "author_counts = features_df['author'].value_counts()\n",
    "multi_doc_authors = author_counts[author_counts >= 2].index[:8]\n",
    "subset = features_df[features_df['author'].isin(multi_doc_authors)]\n",
    "\n",
    "# Plot 1: Sentence length\n",
    "subset.boxplot(column='avg_sentence_length', by='author', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Average Sentence Length')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: Word length\n",
    "subset.boxplot(column='avg_word_length', by='author', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Average Word Length')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 3: Type-token ratio\n",
    "subset.boxplot(column='type_token_ratio', by='author', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Type-Token Ratio (Vocabulary Richness)')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 4: Function word 'the'\n",
    "subset.boxplot(column='fw_the', by='author', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Frequency of \"the\" (%)')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.suptitle('Stylometric Profiles by Author', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Building an Authorship Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for classification\n",
    "# We need authors with multiple documents\n",
    "author_counts = features_df['author'].value_counts()\n",
    "valid_authors = author_counts[author_counts >= 2].index.tolist()\n",
    "\n",
    "# Filter to valid authors and verified documents\n",
    "train_df = features_df[\n",
    "    (features_df['author'].isin(valid_authors)) & \n",
    "    (features_df['authenticity'] == 'verified')\n",
    "].copy()\n",
    "\n",
    "print(f\"Training on {len(train_df)} verified documents from {len(valid_authors)} authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and labels\n",
    "feature_cols = [c for c in features_df.columns \n",
    "                if c not in ['manuscript_id', 'author', 'authenticity']]\n",
    "\n",
    "X = train_df[feature_cols].fillna(0).values\n",
    "y = train_df['author'].values\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train classifier with cross-validation\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Use cross-validation if we have enough data\n",
    "if len(train_df) >= 10:\n",
    "    cv_scores = cross_val_score(clf, X_scaled, y, cv=min(5, len(train_df)//2))\n",
    "    print(f\"Cross-validation accuracy: {cv_scores.mean():.2%} (+/- {cv_scores.std()*2:.2%})\")\n",
    "else:\n",
    "    print(\"Not enough data for cross-validation\")\n",
    "\n",
    "# Train final model on all data\n",
    "clf.fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which features are most important?\n",
    "importances = pd.Series(clf.feature_importances_, index=feature_cols)\n",
    "top_features = importances.nlargest(15)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "top_features.plot(kind='barh', ax=ax, color='steelblue')\n",
    "ax.set_xlabel('Feature Importance')\n",
    "ax.set_title('Most Important Features for Authorship Attribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Testing Disputed Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find suspected forgeries\n",
    "disputed = features_df[features_df['authenticity'] == 'suspected_forgery'].copy()\n",
    "\n",
    "print(f\"Suspected forgeries: {len(disputed)}\")\n",
    "if len(disputed) > 0:\n",
    "    print(disputed[['manuscript_id', 'author']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(disputed) > 0:\n",
    "    # Predict authorship of disputed documents\n",
    "    X_disputed = disputed[feature_cols].fillna(0).values\n",
    "    X_disputed_scaled = scaler.transform(X_disputed)\n",
    "    \n",
    "    # Get predictions and probabilities\n",
    "    predictions = clf.predict(X_disputed_scaled)\n",
    "    probabilities = clf.predict_proba(X_disputed_scaled)\n",
    "    \n",
    "    print(\"\\nAuthorship analysis of suspected forgeries:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for i, (_, row) in enumerate(disputed.iterrows()):\n",
    "        print(f\"\\nDocument: {row['manuscript_id']}\")\n",
    "        print(f\"  Claimed author: {row['author']}\")\n",
    "        print(f\"  Predicted author: {predictions[i]}\")\n",
    "        print(f\"  Confidence: {max(probabilities[i]):.1%}\")\n",
    "        \n",
    "        # Show top 3 most likely authors\n",
    "        prob_ranking = sorted(zip(clf.classes_, probabilities[i]), \n",
    "                             key=lambda x: -x[1])[:3]\n",
    "        print(\"  Top candidates:\")\n",
    "        for author, prob in prob_ranking:\n",
    "            marker = \" <- claimed\" if author == row['author'] else \"\"\n",
    "            print(f\"    {author}: {prob:.1%}{marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Comparing Authentic vs. Disputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have disputed Grigsu documents, compare them to authentic Grigsu\n",
    "if len(disputed) > 0:\n",
    "    # Get any documents attributed to Grigsu (authentic or disputed)\n",
    "    grigsu_authentic = features_df[\n",
    "        (features_df['author'] == 'Grigsu Haldo') & \n",
    "        (features_df['authenticity'] == 'verified')\n",
    "    ]\n",
    "    \n",
    "    grigsu_disputed = features_df[\n",
    "        (features_df['author'].str.contains('Grigsu', na=False)) & \n",
    "        (features_df['authenticity'] == 'suspected_forgery')\n",
    "    ]\n",
    "    \n",
    "    if len(grigsu_authentic) > 0 and len(grigsu_disputed) > 0:\n",
    "        print(f\"Authentic Grigsu documents: {len(grigsu_authentic)}\")\n",
    "        print(f\"Disputed 'Grigsu' documents: {len(grigsu_disputed)}\")\n",
    "        \n",
    "        # Compare key features\n",
    "        compare_features = ['avg_sentence_length', 'avg_word_length', \n",
    "                           'type_token_ratio', 'fw_the', 'fw_that', 'fw_is']\n",
    "        \n",
    "        print(\"\\nFeature comparison:\")\n",
    "        print(f\"{'Feature':<25} {'Authentic':<12} {'Disputed':<12} {'Difference':<12}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for feat in compare_features:\n",
    "            auth_mean = grigsu_authentic[feat].mean()\n",
    "            disp_mean = grigsu_disputed[feat].mean()\n",
    "            diff = disp_mean - auth_mean\n",
    "            print(f\"{feat:<25} {auth_mean:<12.3f} {disp_mean:<12.3f} {diff:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "1. **Stylometric features**: Sentence length, word length, vocabulary richness, function words\n",
    "2. **Author profiling**: Comparing stylistic features across authors\n",
    "3. **Classification**: Training a model to identify authors\n",
    "4. **Forgery detection**: Applying the model to disputed documents\n",
    "\n",
    "### The Evidence So Far\n",
    "\n",
    "Our stylometric analysis provides evidence about disputed manuscripts:\n",
    "- Do they match the claimed author's style?\n",
    "- Which author do they actually resemble?\n",
    "- What specific features differ?\n",
    "\n",
    "This is one piece of evidence. Combined with vocabulary analysis (Tutorial 6), n-gram patterns (Tutorial 4), and historical evidence, we can build a case for or against authenticity.\n",
    "\n",
    "---\n",
    "\n",
    "*\"Every writer has a voice,\" the Chief says, examining your analysis. \"The way they breathe between sentences. The words they reach for without thinking. The forger can copy ideas, but copying the breath? That's much harder.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 7.1: Additional Features\n",
    "Add more stylometric features: contraction usage, sentence starters, comma frequency. Do they improve classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.2: Different Classifiers\n",
    "Try different classification algorithms (SVM, Naive Bayes, Neural Network). Which performs best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.3: School Attribution\n",
    "Instead of author, try to classify documents by philosophical school. Is it easier or harder than author attribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}