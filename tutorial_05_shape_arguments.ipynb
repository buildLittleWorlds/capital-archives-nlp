{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5: The Shape of Arguments\n",
    "\n",
    "## The Capital Archives — A Course in Natural Language Processing\n",
    "\n",
    "---\n",
    "\n",
    "*The water-school scholars believe that the structure of an argument reveals its maker's temperament. \"A scholar who uses many nouns,\" Yasho wrote, \"thinks in objects, in discrete entities. A scholar who favors verbs thinks in processes, in transformation. The grammar is the philosophy.\"*\n",
    "\n",
    "*The Chief Archivist wants you to analyze the grammatical structure of the debate transcripts. Who uses more nouns? More verbs? More questions?*\n",
    "\n",
    "---\n",
    "\n",
    "In this tutorial, you will learn:\n",
    "- Part-of-speech (POS) tagging\n",
    "- Sentence structure analysis\n",
    "- Comparing grammatical patterns across texts\n",
    "- Using spaCy for advanced NLP"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# COLAB SETUP - Run this cell first!\n# ============================================\n# This cell sets up the environment for Google Colab\n# Skip this cell if running locally\n\nimport os\n\n# Clone the repository if running in Colab\nif 'google.colab' in str(get_ipython()):\n    if not os.path.exists('capital-archives-nlp'):\n        !git clone https://github.com/buildLittleWorlds/capital-archives-nlp.git\n    os.chdir('capital-archives-nlp')\n    \n    # Install/download NLTK data\n    import nltk\n    nltk.download('punkt', quiet=True)\n    nltk.download('punkt_tab', quiet=True)\n    nltk.download('averaged_perceptron_tagger', quiet=True)\n    nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n    print(\"✓ Repository cloned and NLTK data downloaded!\")\nelse:\n    print(\"✓ Running locally - no setup needed\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "print(\"Libraries loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load spaCy (optional but recommended)\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    SPACY_AVAILABLE = True\n",
    "    print(\"spaCy loaded successfully.\")\n",
    "except:\n",
    "    SPACY_AVAILABLE = False\n",
    "    print(\"spaCy not available. Using NLTK only.\")\n",
    "    print(\"To install: pip install spacy && python -m spacy download en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus\n",
    "manuscripts = pd.read_csv('data/manuscripts.csv')\n",
    "texts = pd.read_csv('data/manuscript_texts.csv')\n",
    "\n",
    "corpus = texts.groupby('manuscript_id').agg(\n",
    "    text=('text', ' '.join)\n",
    ").reset_index()\n",
    "\n",
    "corpus = corpus.merge(\n",
    "    manuscripts[['manuscript_id', 'title', 'author', 'genre']],\n",
    "    on='manuscript_id', how='left'\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(corpus)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 What is Part-of-Speech Tagging?\n",
    "\n",
    "**Part-of-speech (POS) tagging** assigns grammatical categories to words:\n",
    "- **NN** = noun\n",
    "- **VB** = verb\n",
    "- **JJ** = adjective\n",
    "- **RB** = adverb\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic POS tagging with NLTK\n",
    "sample = \"Grigsu argued that words persist like stones in the darkness.\"\n",
    "tokens = word_tokenize(sample)\n",
    "tagged = pos_tag(tokens)\n",
    "\n",
    "print(\"POS-tagged sentence:\")\n",
    "for word, tag in tagged:\n",
    "    print(f\"  {word}: {tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tag reference\n",
    "pos_explanations = {\n",
    "    'NN': 'noun, singular',\n",
    "    'NNS': 'noun, plural',\n",
    "    'NNP': 'proper noun, singular',\n",
    "    'VB': 'verb, base form',\n",
    "    'VBD': 'verb, past tense',\n",
    "    'VBG': 'verb, gerund/present participle',\n",
    "    'VBN': 'verb, past participle',\n",
    "    'VBP': 'verb, present, not 3rd person singular',\n",
    "    'VBZ': 'verb, present, 3rd person singular',\n",
    "    'JJ': 'adjective',\n",
    "    'JJR': 'adjective, comparative',\n",
    "    'JJS': 'adjective, superlative',\n",
    "    'RB': 'adverb',\n",
    "    'IN': 'preposition/subordinating conjunction',\n",
    "    'DT': 'determiner',\n",
    "    'PRP': 'personal pronoun',\n",
    "}\n",
    "\n",
    "print(\"Common POS tags:\")\n",
    "for tag, explanation in pos_explanations.items():\n",
    "    print(f\"  {tag}: {explanation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 POS Tagging a Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_distribution(text):\n",
    "    \"\"\"\n",
    "    Get the distribution of POS tags in a text.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Counter : Counts of each POS tag\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    tags = [tag for _, tag in tagged]\n",
    "    return Counter(tags)\n",
    "\n",
    "# Test on one document\n",
    "sample_doc = corpus[corpus['author'] == 'Grigsu Haldo'].iloc[0]\n",
    "pos_dist = get_pos_distribution(sample_doc['text'])\n",
    "\n",
    "print(f\"POS distribution in '{sample_doc['title'][:40]}...':\")\n",
    "for tag, count in pos_dist.most_common(15):\n",
    "    print(f\"  {tag}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate noun/verb ratio\n",
    "def noun_verb_ratio(text):\n",
    "    \"\"\"\n",
    "    Calculate the ratio of nouns to verbs.\n",
    "    Higher ratio = more noun-heavy (\"thingness\")\n",
    "    Lower ratio = more verb-heavy (\"process\")\n",
    "    \"\"\"\n",
    "    pos_dist = get_pos_distribution(text)\n",
    "    \n",
    "    nouns = sum(pos_dist.get(tag, 0) for tag in ['NN', 'NNS', 'NNP', 'NNPS'])\n",
    "    verbs = sum(pos_dist.get(tag, 0) for tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'])\n",
    "    \n",
    "    if verbs == 0:\n",
    "        return float('inf')\n",
    "    return nouns / verbs\n",
    "\n",
    "# Test\n",
    "print(f\"Noun/verb ratio: {noun_verb_ratio(sample_doc['text']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Comparing Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate POS statistics for each document\n",
    "def analyze_document_grammar(text):\n",
    "    \"\"\"\n",
    "    Analyze grammatical features of a text.\n",
    "    \"\"\"\n",
    "    pos_dist = get_pos_distribution(text)\n",
    "    total = sum(pos_dist.values())\n",
    "    \n",
    "    if total == 0:\n",
    "        return {}\n",
    "    \n",
    "    nouns = sum(pos_dist.get(tag, 0) for tag in ['NN', 'NNS', 'NNP', 'NNPS'])\n",
    "    verbs = sum(pos_dist.get(tag, 0) for tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'])\n",
    "    adjectives = sum(pos_dist.get(tag, 0) for tag in ['JJ', 'JJR', 'JJS'])\n",
    "    adverbs = sum(pos_dist.get(tag, 0) for tag in ['RB', 'RBR', 'RBS'])\n",
    "    \n",
    "    return {\n",
    "        'noun_pct': nouns / total * 100,\n",
    "        'verb_pct': verbs / total * 100,\n",
    "        'adj_pct': adjectives / total * 100,\n",
    "        'adv_pct': adverbs / total * 100,\n",
    "        'noun_verb_ratio': nouns / verbs if verbs > 0 else np.nan\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze all documents\n",
    "grammar_stats = []\n",
    "for _, row in corpus.iterrows():\n",
    "    stats = analyze_document_grammar(row['text'])\n",
    "    stats['manuscript_id'] = row['manuscript_id']\n",
    "    stats['author'] = row['author']\n",
    "    stats['genre'] = row['genre']\n",
    "    grammar_stats.append(stats)\n",
    "\n",
    "grammar_df = pd.DataFrame(grammar_stats)\n",
    "grammar_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare authors\n",
    "author_grammar = grammar_df.groupby('author').agg({\n",
    "    'noun_pct': 'mean',\n",
    "    'verb_pct': 'mean',\n",
    "    'adj_pct': 'mean',\n",
    "    'noun_verb_ratio': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"Grammatical profiles by author:\")\n",
    "print(author_grammar.sort_values('noun_verb_ratio', ascending=False).head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Noun vs verb percentages by author\n",
    "top_authors = grammar_df.groupby('author').size().nlargest(10).index\n",
    "author_subset = grammar_df[grammar_df['author'].isin(top_authors)]\n",
    "\n",
    "author_means = author_subset.groupby('author')[['noun_pct', 'verb_pct']].mean()\n",
    "author_means.plot(kind='bar', ax=axes[0], color=['steelblue', 'coral'])\n",
    "axes[0].set_xlabel('Author')\n",
    "axes[0].set_ylabel('Percentage of words')\n",
    "axes[0].set_title('Noun and Verb Usage by Author')\n",
    "axes[0].legend(['Nouns', 'Verbs'])\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Noun/verb ratio distribution\n",
    "axes[1].hist(grammar_df['noun_verb_ratio'].dropna(), bins=20, color='steelblue', edgecolor='white')\n",
    "axes[1].set_xlabel('Noun/Verb Ratio')\n",
    "axes[1].set_ylabel('Number of Documents')\n",
    "axes[1].set_title('Distribution of Noun/Verb Ratios')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Sentence-Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentences(text):\n",
    "    \"\"\"\n",
    "    Analyze sentence-level features.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    if len(sentences) == 0:\n",
    "        return {}\n",
    "    \n",
    "    sentence_lengths = [len(word_tokenize(s)) for s in sentences]\n",
    "    \n",
    "    # Count questions\n",
    "    questions = sum(1 for s in sentences if s.strip().endswith('?'))\n",
    "    \n",
    "    # Count exclamations\n",
    "    exclamations = sum(1 for s in sentences if s.strip().endswith('!'))\n",
    "    \n",
    "    return {\n",
    "        'num_sentences': len(sentences),\n",
    "        'avg_sentence_length': np.mean(sentence_lengths),\n",
    "        'std_sentence_length': np.std(sentence_lengths),\n",
    "        'max_sentence_length': max(sentence_lengths),\n",
    "        'min_sentence_length': min(sentence_lengths),\n",
    "        'question_ratio': questions / len(sentences),\n",
    "        'exclamation_ratio': exclamations / len(sentences)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sentences for all documents\n",
    "sentence_stats = []\n",
    "for _, row in corpus.iterrows():\n",
    "    stats = analyze_sentences(row['text'])\n",
    "    stats['manuscript_id'] = row['manuscript_id']\n",
    "    stats['author'] = row['author']\n",
    "    stats['genre'] = row['genre']\n",
    "    sentence_stats.append(stats)\n",
    "\n",
    "sentence_df = pd.DataFrame(sentence_stats)\n",
    "sentence_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Who writes the longest sentences?\n",
    "print(\"Average sentence length by author:\")\n",
    "author_sentences = sentence_df.groupby('author')['avg_sentence_length'].mean().sort_values(ascending=False)\n",
    "print(author_sentences.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which genres use more questions?\n",
    "print(\"\\nQuestion ratio by genre:\")\n",
    "genre_questions = sentence_df.groupby('genre')['question_ratio'].mean().sort_values(ascending=False)\n",
    "print(genre_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Using spaCy for Advanced Analysis\n",
    "\n",
    "spaCy provides more sophisticated linguistic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPACY_AVAILABLE:\n",
    "    # Process a sample text with spaCy\n",
    "    sample = \"Grigsu argued that words persist like stones, but Yasho believed they dissolve.\"\n",
    "    doc = nlp(sample)\n",
    "    \n",
    "    print(\"spaCy analysis:\")\n",
    "    for token in doc:\n",
    "        print(f\"  {token.text:12} {token.pos_:6} {token.dep_:10} {token.head.text}\")\n",
    "else:\n",
    "    print(\"spaCy not available. Install it for advanced analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPACY_AVAILABLE:\n",
    "    # Named entity recognition\n",
    "    sample = \"Grigsu traveled from the Capital to Yeller Quarry with Yasho and Bagbu in 869.\"\n",
    "    doc = nlp(sample)\n",
    "    \n",
    "    print(\"Named entities:\")\n",
    "    for ent in doc.ents:\n",
    "        print(f\"  {ent.text}: {ent.label_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPACY_AVAILABLE:\n",
    "    def spacy_pos_analysis(text, max_length=50000):\n",
    "        \"\"\"\n",
    "        Analyze POS distribution using spaCy.\n",
    "        \"\"\"\n",
    "        # Truncate if too long\n",
    "        if len(text) > max_length:\n",
    "            text = text[:max_length]\n",
    "        \n",
    "        doc = nlp(text)\n",
    "        pos_counts = Counter(token.pos_ for token in doc)\n",
    "        return pos_counts\n",
    "    \n",
    "    # Test\n",
    "    sample_doc = corpus.iloc[0]\n",
    "    pos_counts = spacy_pos_analysis(sample_doc['text'])\n",
    "    \n",
    "    print(\"POS distribution (spaCy):\")\n",
    "    for pos, count in pos_counts.most_common(10):\n",
    "        print(f\"  {pos}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Combining Features for Author Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine grammar and sentence stats\n",
    "full_stats = grammar_df.merge(sentence_df, on=['manuscript_id', 'author', 'genre'])\n",
    "\n",
    "# Create author profiles\n",
    "author_profiles = full_stats.groupby('author').agg({\n",
    "    'noun_pct': 'mean',\n",
    "    'verb_pct': 'mean',\n",
    "    'adj_pct': 'mean',\n",
    "    'noun_verb_ratio': 'mean',\n",
    "    'avg_sentence_length': 'mean',\n",
    "    'question_ratio': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"Author grammatical profiles:\")\n",
    "print(author_profiles.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "1. **POS tagging**: Assigning grammatical categories to words\n",
    "2. **POS distribution analysis**: Counting different word types\n",
    "3. **Noun/verb ratio**: A measure of \"thingness\" vs \"process\"\n",
    "4. **Sentence analysis**: Length, questions, exclamations\n",
    "5. **Author profiling**: Combining features to characterize writers\n",
    "\n",
    "### What the Grammar Reveals\n",
    "\n",
    "Different authors have distinct grammatical signatures:\n",
    "- Some favor nouns (emphasizing objects, concepts)\n",
    "- Some favor verbs (emphasizing actions, processes)\n",
    "- Sentence length varies by author and genre\n",
    "\n",
    "These patterns can help with authorship attribution and genre classification.\n",
    "\n",
    "---\n",
    "\n",
    "*Yasho's hypothesis seems to hold: the grammar does reveal something about the writer. Grigsu, with his emphasis on permanence and stone, uses more nouns—the grammatical category for things that persist. The water-school writers favor verbs—the category for change and motion.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 5.1: Adjective Analysis\n",
    "Which authors use the most adjectives? Extract the actual adjectives used by each author and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.2: Debate Questions\n",
    "Analyze the debate transcripts specifically. Who asks more questions? Is there a correlation between questions asked and debate outcomes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.3: Sentence Complexity\n",
    "Use spaCy (if available) to analyze dependency structures. Which authors write more complex sentences (deeper dependency trees)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}