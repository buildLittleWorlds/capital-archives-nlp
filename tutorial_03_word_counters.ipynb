{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: The Word Counters\n",
    "\n",
    "## The Capital Archives — A Course in Natural Language Processing\n",
    "\n",
    "---\n",
    "\n",
    "*The stone-school scholars believe that counting words reveals hidden patterns in reality. \"The primes appear everywhere,\" Grigsu wrote, \"in Yeller's movements, in the counting songs, in the very structure of language itself.\" Whether or not words are permanent, counting them is certainly useful.*\n",
    "\n",
    "*The Chief Archivist wants frequency reports on the collection. Which words appear most often? How do different authors and genres differ in their word usage?*\n",
    "\n",
    "---\n",
    "\n",
    "In this tutorial, you will learn:\n",
    "- Tokenization: breaking text into words\n",
    "- Word frequency analysis\n",
    "- Stopword removal\n",
    "- Comparing word frequencies across documents\n",
    "- Visualizing word distributions"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# COLAB SETUP - Run this cell first!\n# ============================================\n# This cell sets up the environment for Google Colab\n# Skip this cell if running locally\n\nimport os\n\n# Clone the repository if running in Colab\nif 'google.colab' in str(get_ipython()):\n    if not os.path.exists('capital-archives-nlp'):\n        !git clone https://github.com/buildLittleWorlds/capital-archives-nlp.git\n    os.chdir('capital-archives-nlp')\n    \n    # Install/download NLTK data\n    import nltk\n    nltk.download('punkt', quiet=True)\n    nltk.download('punkt_tab', quiet=True)\n    nltk.download('stopwords', quiet=True)\n    print(\"✓ Repository cloned and NLTK data downloaded!\")\nelse:\n    print(\"✓ Running locally - no setup needed\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK data (run once)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "print(\"Libraries loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our data\n",
    "manuscripts = pd.read_csv('manuscripts.csv')\n",
    "texts = pd.read_csv('manuscript_texts.csv')\n",
    "\n",
    "# Create a combined corpus\n",
    "corpus = texts.groupby('manuscript_id').agg(\n",
    "    text=('text', ' '.join)\n",
    ").reset_index()\n",
    "\n",
    "corpus = corpus.merge(\n",
    "    manuscripts[['manuscript_id', 'title', 'author', 'genre', 'authenticity_status']],\n",
    "    on='manuscript_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(corpus)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 What is Tokenization?\n",
    "\n",
    "**Tokenization** is the process of breaking text into smaller units called **tokens**. Usually, tokens are words, but they can also be sentences, paragraphs, or even individual characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple tokenization: split on whitespace\n",
    "sample_text = \"Grigsu argued that words are hard, like stones. Yasho disagreed—she believed in dissolution.\"\n",
    "\n",
    "simple_tokens = sample_text.split()\n",
    "print(\"Simple split():\")\n",
    "print(simple_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: punctuation is attached to words!\n",
    "# 'hard,' and 'stones.' include punctuation\n",
    "# 'disagreed—she' is treated as one token\n",
    "\n",
    "# Better: use NLTK's word_tokenize\n",
    "nltk_tokens = word_tokenize(sample_text)\n",
    "print(\"\\nNLTK word_tokenize():\")\n",
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK separates punctuation from words\n",
    "# But notice: it keeps punctuation as separate tokens\n",
    "# We might want to remove them\n",
    "\n",
    "# Filter to keep only alphabetic tokens\n",
    "word_tokens = [token for token in nltk_tokens if token.isalpha()]\n",
    "print(\"\\nAlphabetic tokens only:\")\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Tokenization Function\n",
    "\n",
    "Let's create a reusable tokenization function for our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, lowercase=True, remove_punctuation=True):\n",
    "    \"\"\"\n",
    "    Tokenize text into words.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        The text to tokenize\n",
    "    lowercase : bool\n",
    "        Convert tokens to lowercase\n",
    "    remove_punctuation : bool\n",
    "        Remove non-alphabetic tokens\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list : List of tokens\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(str(text))\n",
    "    \n",
    "    # Lowercase\n",
    "    if lowercase:\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "    \n",
    "    # Remove punctuation\n",
    "    if remove_punctuation:\n",
    "        tokens = [t for t in tokens if t.isalpha()]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Test it\n",
    "print(tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize our entire corpus\n",
    "corpus['tokens'] = corpus['text'].apply(tokenize)\n",
    "corpus['token_count'] = corpus['tokens'].apply(len)\n",
    "\n",
    "print(f\"Total tokens in corpus: {corpus['token_count'].sum():,}\")\n",
    "print(f\"Average tokens per document: {corpus['token_count'].mean():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Word Frequency Analysis\n",
    "\n",
    "Now let's count how often each word appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all tokens from all documents\n",
    "all_tokens = []\n",
    "for tokens in corpus['tokens']:\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "print(f\"Total tokens: {len(all_tokens):,}\")\n",
    "print(f\"Unique tokens (vocabulary): {len(set(all_tokens)):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count word frequencies\n",
    "word_freq = Counter(all_tokens)\n",
    "\n",
    "print(\"25 most common words:\")\n",
    "for word, count in word_freq.most_common(25):\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "The most common words are **function words** like \"the\", \"is\", \"of\", \"and\". These words appear frequently in all English text—they don't tell us much about the specific content of our manuscripts.\n",
    "\n",
    "These are called **stopwords**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Stopword Removal\n",
    "\n",
    "**Stopwords** are common words that carry little semantic meaning. Removing them helps us focus on the content words that distinguish one text from another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK's English stopwords\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "print(f\"Number of English stopwords: {len(english_stopwords)}\")\n",
    "print(f\"\\nSample stopwords: {sorted(english_stopwords)[:30]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords from our word frequency\n",
    "content_words = {word: count for word, count in word_freq.items() \n",
    "                 if word not in english_stopwords}\n",
    "\n",
    "print(\"25 most common content words (stopwords removed):\")\n",
    "for word, count in Counter(content_words).most_common(25):\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see more interesting words! Words like \"words\", \"stone\", \"dissolution\", \"meaning\" tell us about the actual content of the philosophical debates in our archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function that tokenizes with stopword removal\n",
    "def tokenize_content(text, extra_stopwords=None):\n",
    "    \"\"\"\n",
    "    Tokenize text and remove stopwords.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        The text to tokenize\n",
    "    extra_stopwords : set\n",
    "        Additional stopwords to remove\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list : List of content tokens\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    if extra_stopwords:\n",
    "        stop_words.update(extra_stopwords)\n",
    "    \n",
    "    tokens = tokenize(text)  # Our earlier function\n",
    "    content_tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    return content_tokens\n",
    "\n",
    "# Apply to corpus\n",
    "corpus['content_tokens'] = corpus['text'].apply(tokenize_content)\n",
    "corpus['content_count'] = corpus['content_tokens'].apply(len)\n",
    "\n",
    "print(f\"Tokens removed by stopword filtering: {corpus['token_count'].sum() - corpus['content_count'].sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Comparing Word Frequencies Across Groups\n",
    "\n",
    "The real power of word frequency analysis comes from comparison. Do different authors use different words? What about different genres?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_freq(documents, column='content_tokens'):\n",
    "    \"\"\"\n",
    "    Get word frequencies from a set of documents.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    documents : DataFrame\n",
    "        Documents with a tokens column\n",
    "    column : str\n",
    "        Name of the column containing token lists\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Counter : Word frequency counts\n",
    "    \"\"\"\n",
    "    all_tokens = []\n",
    "    for tokens in documents[column]:\n",
    "        all_tokens.extend(tokens)\n",
    "    return Counter(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare word frequencies by author\n",
    "# Let's look at two major philosophers: Grigsu (stone-school) and Yasho (water-school)\n",
    "\n",
    "grigsu_docs = corpus[corpus['author'] == 'Grigsu Haldo']\n",
    "yasho_docs = corpus[corpus['author'] == 'Yasho Krent']\n",
    "\n",
    "print(f\"Grigsu documents: {len(grigsu_docs)}\")\n",
    "print(f\"Yasho documents: {len(yasho_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grigsu_freq = get_word_freq(grigsu_docs)\n",
    "yasho_freq = get_word_freq(yasho_docs)\n",
    "\n",
    "print(\"Grigsu's top 15 words:\")\n",
    "for word, count in grigsu_freq.most_common(15):\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "print(\"\\nYasho's top 15 words:\")\n",
    "for word, count in yasho_freq.most_common(15):\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words that appear much more in Grigsu than Yasho\n",
    "def compare_frequencies(freq1, freq2, name1=\"Group 1\", name2=\"Group 2\", min_count=3):\n",
    "    \"\"\"\n",
    "    Compare word frequencies between two groups.\n",
    "    Returns words that are distinctive to each group.\n",
    "    \"\"\"\n",
    "    # Normalize by total counts\n",
    "    total1 = sum(freq1.values())\n",
    "    total2 = sum(freq2.values())\n",
    "    \n",
    "    all_words = set(freq1.keys()) | set(freq2.keys())\n",
    "    \n",
    "    ratios = []\n",
    "    for word in all_words:\n",
    "        count1 = freq1.get(word, 0)\n",
    "        count2 = freq2.get(word, 0)\n",
    "        \n",
    "        if count1 + count2 < min_count:\n",
    "            continue\n",
    "        \n",
    "        # Normalize to rate per 1000 words\n",
    "        rate1 = (count1 / total1) * 1000 if total1 > 0 else 0\n",
    "        rate2 = (count2 / total2) * 1000 if total2 > 0 else 0\n",
    "        \n",
    "        # Ratio (add smoothing to avoid division by zero)\n",
    "        ratio = (rate1 + 0.1) / (rate2 + 0.1)\n",
    "        \n",
    "        ratios.append((word, count1, count2, rate1, rate2, ratio))\n",
    "    \n",
    "    ratios.sort(key=lambda x: x[5], reverse=True)\n",
    "    return ratios\n",
    "\n",
    "comparison = compare_frequencies(grigsu_freq, yasho_freq, \"Grigsu\", \"Yasho\")\n",
    "\n",
    "print(\"Words MORE common in Grigsu than Yasho:\")\n",
    "for word, c1, c2, r1, r2, ratio in comparison[:10]:\n",
    "    print(f\"  {word}: Grigsu={c1}, Yasho={c2}, ratio={ratio:.2f}\")\n",
    "\n",
    "print(\"\\nWords MORE common in Yasho than Grigsu:\")\n",
    "for word, c1, c2, r1, r2, ratio in comparison[-10:]:\n",
    "    print(f\"  {word}: Grigsu={c1}, Yasho={c2}, ratio={ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Visualizing Word Frequencies\n",
    "\n",
    "Visualizations help us understand word distributions at a glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart of top words\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "top_words = Counter(content_words).most_common(20)\n",
    "words, counts = zip(*top_words)\n",
    "\n",
    "ax.barh(range(len(words)), counts, color='steelblue')\n",
    "ax.set_yticks(range(len(words)))\n",
    "ax.set_yticklabels(words)\n",
    "ax.invert_yaxis()  # Most common at top\n",
    "ax.set_xlabel('Frequency')\n",
    "ax.set_title('Most Common Content Words in the Archive')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud (if wordcloud is installed)\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "    \n",
    "    wc = WordCloud(width=800, height=400, background_color='white', \n",
    "                   max_words=100, colormap='viridis')\n",
    "    wc.generate_from_frequencies(content_words)\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Cloud of Archive Content')\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"WordCloud not installed. Run: pip install wordcloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two authors visually\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Grigsu\n",
    "grigsu_top = grigsu_freq.most_common(15)\n",
    "words_g, counts_g = zip(*grigsu_top)\n",
    "axes[0].barh(range(len(words_g)), counts_g, color='darkred')\n",
    "axes[0].set_yticks(range(len(words_g)))\n",
    "axes[0].set_yticklabels(words_g)\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].set_xlabel('Frequency')\n",
    "axes[0].set_title('Grigsu Haldo (Stone-School)')\n",
    "\n",
    "# Yasho\n",
    "yasho_top = yasho_freq.most_common(15)\n",
    "words_y, counts_y = zip(*yasho_top)\n",
    "axes[1].barh(range(len(words_y)), counts_y, color='darkblue')\n",
    "axes[1].set_yticks(range(len(words_y)))\n",
    "axes[1].set_yticklabels(words_y)\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].set_xlabel('Frequency')\n",
    "axes[1].set_title('Yasho Krent (Water-School)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Zipf's Law\n",
    "\n",
    "**Zipf's Law** states that in any natural language corpus, the frequency of a word is inversely proportional to its rank. The most common word appears about twice as often as the second most common, three times as often as the third, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if our corpus follows Zipf's Law\n",
    "sorted_freq = sorted(word_freq.values(), reverse=True)\n",
    "ranks = range(1, len(sorted_freq) + 1)\n",
    "\n",
    "# Plot on log-log scale\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.loglog(ranks, sorted_freq, 'b.', markersize=3, alpha=0.5)\n",
    "ax.set_xlabel('Rank (log scale)')\n",
    "ax.set_ylabel('Frequency (log scale)')\n",
    "ax.set_title(\"Zipf's Law: Word Frequency vs. Rank\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# If it follows Zipf's Law, this should be roughly a straight line\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Genre-Specific Vocabulary\n",
    "\n",
    "Let's compare vocabulary across different genres in the archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What genres do we have?\n",
    "print(\"Documents by genre:\")\n",
    "print(corpus['genre'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word frequencies for different genres\n",
    "genre_freqs = {}\n",
    "for genre in corpus['genre'].unique():\n",
    "    if pd.notna(genre):\n",
    "        genre_docs = corpus[corpus['genre'] == genre]\n",
    "        if len(genre_docs) > 0:\n",
    "            genre_freqs[genre] = get_word_freq(genre_docs)\n",
    "\n",
    "# Show top words for each genre\n",
    "for genre, freq in genre_freqs.items():\n",
    "    print(f\"\\n{genre.upper()} (top 10 words):\")\n",
    "    for word, count in freq.most_common(10):\n",
    "        print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Searching for Specific Terms\n",
    "\n",
    "Sometimes we want to know how specific terms are distributed across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_term(corpus_df, term, token_col='content_tokens'):\n",
    "    \"\"\"\n",
    "    Find all documents containing a specific term.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame : Documents containing the term, with count\n",
    "    \"\"\"\n",
    "    term = term.lower()\n",
    "    \n",
    "    results = []\n",
    "    for _, row in corpus_df.iterrows():\n",
    "        count = row[token_col].count(term)\n",
    "        if count > 0:\n",
    "            results.append({\n",
    "                'manuscript_id': row['manuscript_id'],\n",
    "                'title': row['title'],\n",
    "                'author': row['author'],\n",
    "                'count': count\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results).sort_values('count', ascending=False)\n",
    "\n",
    "# Search for key terms\n",
    "print(\"Documents mentioning 'dissolution':\")\n",
    "print(search_term(corpus, 'dissolution'))\n",
    "\n",
    "print(\"\\nDocuments mentioning 'stone':\")\n",
    "print(search_term(corpus, 'stone'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track multiple terms\n",
    "key_terms = ['stone', 'water', 'word', 'meaning', 'dissolution', 'permanent', 'pool']\n",
    "\n",
    "term_distribution = {}\n",
    "for term in key_terms:\n",
    "    term_counts = []\n",
    "    for tokens in corpus['content_tokens']:\n",
    "        term_counts.append(tokens.count(term.lower()))\n",
    "    term_distribution[term] = sum(term_counts)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "terms, counts = zip(*sorted(term_distribution.items(), key=lambda x: -x[1]))\n",
    "ax.bar(terms, counts, color='steelblue')\n",
    "ax.set_xlabel('Term')\n",
    "ax.set_ylabel('Total Occurrences')\n",
    "ax.set_title('Key Term Frequencies in the Archive')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "1. **Tokenization**: Breaking text into words using `word_tokenize()`\n",
    "2. **Word frequency**: Counting words with `Counter`\n",
    "3. **Stopword removal**: Filtering out common function words\n",
    "4. **Frequency comparison**: Finding words distinctive to different groups\n",
    "5. **Visualization**: Bar charts, word clouds, Zipf's Law\n",
    "\n",
    "### Key Insights from the Archive\n",
    "\n",
    "Our word frequency analysis has already revealed important patterns:\n",
    "- The archive is dominated by philosophical vocabulary (\"word\", \"meaning\", \"stone\", \"water\")\n",
    "- Different authors have distinctive vocabularies\n",
    "- Genre affects word choice (treatises vs. expedition reports)\n",
    "\n",
    "---\n",
    "\n",
    "*The stone-school archivist looks at your frequency reports with satisfaction. \"You see?\" he says. \"The patterns are there, waiting to be counted. The primes in the word frequencies. The structures in the silence between words.\" You're not sure you see primes in your bar charts, but you nod politely.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 3.1: Hapax Legomena\n",
    "A **hapax legomenon** is a word that appears only once in a corpus. Find all hapax legomena in our archive. What proportion of the vocabulary are hapax legomena? What kinds of words are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "hapax = [word for word, count in word_freq.items() if count == 1]\n",
    "print(f\"Hapax legomena: {len(hapax)} words ({100*len(hapax)/len(word_freq):.1f}% of vocabulary)\")\n",
    "print(f\"\\nSample hapax legomena: {hapax[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Vocabulary Richness\n",
    "Calculate the **type-token ratio** (TTR) for each author. TTR = unique words / total words. Which authors have the richest vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3: Custom Stopwords\n",
    "Some words specific to our archive (like \"manuscript\", \"archive\", \"scholar\") might be too common to be useful. Create a custom stopword list for the archive and see how it changes the top content words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "archive_stopwords = {'manuscript', 'archive', 'scholar', 'text', 'page', 'document'}\n",
    "# Add to existing stopwords and re-analyze..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}